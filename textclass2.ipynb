{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules\n",
    "- - - - - - - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os, sys\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = 4000                  # global variable - the number of words in the vocabulary (feature set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit(X, Y):\n",
    "\n",
    "  * **X** - X  training  (numpy array, shape = (16000, 4000))\n",
    "  * **Y** - Y training (numpy array, shape = (16000, 1))\n",
    "  * It forms a 2 level dictionary using **X, Y**\n",
    "  * Level 1:\n",
    "  >  + **current_class**\n",
    "  >  + **total_docs** = 16000\n",
    "\n",
    "  * Level 2:     \n",
    "          \n",
    "       for each *current_class*\n",
    "     \n",
    "       > + 4000(*num_of_features*) keys, each storing the frequency of that word in feature set\n",
    "       > + key: **total_class_docs** = 800\n",
    "       > + key: **total_words** = total words (of the vocabulary) in the *current_class*\n",
    "\n",
    "\n",
    "   * Returns **result** : the dictionary hence built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y):\n",
    "    \n",
    "    result = {}                                                     # initialise dictionary\n",
    "    result[\"total_docs\"] = 0                                        # total number of documents in training set = 16000\n",
    "    \n",
    "    # getting unique class values\n",
    "    class_values = set()\n",
    "    for i in range(1, Y.shape[0]):                                  # Y.shape = (16000, 1)\n",
    "        class_values.add(Y[i][0])\n",
    "\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}                                  # level 1: class name\n",
    "       \n",
    "        # generating T/F array, if the current_class matches or not\n",
    "        current_class_rows = []\n",
    "        for i in Y:\n",
    "            current_class_rows.append(i[0] == current_class)\n",
    "\n",
    "        X_current = X[current_class_rows]                           # rows in X, where Y == current_class\n",
    "        Y_current = Y[current_class_rows]                           # rows in Y, where Y == current_class\n",
    "        \n",
    "        X_current = np.array(X_current)                             # convert to numpy arrays\n",
    "        Y_current = np.array(Y_current)\n",
    "\n",
    "        # Y_current.shape = (800, ) => 800 rows of each class\n",
    "        \n",
    "        result[\"total_docs\"]  = result[\"total_docs\"] + Y_current.shape[0]\n",
    "        result[current_class][\"total_class_docs\"] = Y_current.shape[0]\n",
    "        result[current_class][\"total_words\"] = 0\n",
    "        \n",
    "        for j in range(num_of_features):                                \n",
    "            result[current_class][j] = X_current[:, j].sum()        # level 2: frequency\n",
    "            result[current_class][\"total_words\"] += result[current_class][j]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### get_common_words(topX, text)\n",
    "  >  * **topX** -  number of top words to select\n",
    "  >  * **text** - \n",
    " > + array of strings\n",
    " > + from which top words are to be selected\n",
    " >  * returns **common_words**: \n",
    " > + array of tuples : (word, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_words(topX, text):\n",
    "    \n",
    "    freq = nltk.FreqDist(text)                                      # using nltk.FreqDist() to get [(word, freq)..]\n",
    "    common_words =  freq.most_common(topX) \n",
    "    \n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_data(parent_directory):\n",
    "\n",
    "  > * **parent_directory** -  parent directory containing the sub directiories and data files\n",
    "  > * traverses the **files** by using the *path* specified\n",
    "  > * creates a **text** array of strings having all the words in all the files\n",
    "  > * the file **content** is word tokenized before adding to **text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(parent_directory):\n",
    "    \n",
    "    dirs = os.listdir(parent_directory)                       #list of sub directories in parent directory\n",
    "    text = []                                                 # empty list\n",
    "    \n",
    "    for current_dir_index in range(1, len(dirs)+1):           # for each sub-directory in the parent directory\n",
    "        path = '20_newsgroups\\\\' + dirs[current_dir_index-1]  # finding the path\n",
    "        files = os.listdir(path)                              # files inside the subdirectory\n",
    "        \n",
    "        for file in files[0: 800]:                            # using first 800 files in each subdirectory for training\n",
    "            path_of_file_in_directory = path + '\\\\' + file\n",
    "            ptr = open(path_of_file_in_directory)             # open() returns a pointer\n",
    "            content = ptr.read()\n",
    "            text += word_tokenize(content)                    # word-tokenized content\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_features(most_common_words)\n",
    "   > * finds the feature set from **most_common_words**\n",
    "   > * **most_common_words** :  \n",
    "    + array of tuples: (word, frequency)  \n",
    "    + sorted in increasing order of frequency\n",
    "    > * returns **features** : array of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(most_common_words):\n",
    "    \n",
    "    features = []                                                       # initialize empty list\n",
    "    for word in most_common_words:                            \n",
    "        features.append(word[0])                                        # append the words\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_simple_pos(tag):\n",
    "  > * converts the POS **tag** generated by *pos_tag* into the format used by **lemmatize()**\n",
    "  > * if else statements used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    \n",
    "    if tag.startswith('J'):                                            # adjective\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    elif tag.startswith('N'):                                          # noun\n",
    "        return wordnet.NOUN \n",
    "    \n",
    "    elif tag.startswith('V'):                                          # verb\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    elif tag.startswith('R'):                                          # adverb\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stopwords():\n",
    "  > * returns a list of **stopwords**\n",
    "  > * uses the inbuilt *English* stopwords and punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    \n",
    "    stops = stopwords.words('English')\n",
    "    punctuations = string.punctuation                                  \n",
    "    stops = stops + list(punctuations)                                 # concatenation of lists\n",
    "    stops = stops + list(\"'s\")\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatize_text(words):\n",
    "  > * **lemmatize**s the words using **WordNetLemmatizer()**\n",
    "  > * **words** : array of words\n",
    "  > * returns **output_words** : array of lemmatized strings in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(words):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    output_words = []\n",
    "    \n",
    "    for w in words:                                                   # lemmatizes each word one by one\n",
    "        if w.lower() not in stops:\n",
    "            pos = pos_tag([w])                                        # find POS\n",
    "            \n",
    "            clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1])) \n",
    "            \n",
    "            output_words.append(clean_word.lower())                   # append the lemmatized strings in lowercase\n",
    "            \n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### makeXY(features, parent_directory, stops)\n",
    "  * **features** - array of words to be used as *features*\n",
    "  * **parent_directory** - parent directory containing the sub directiories and data files\n",
    "  * **stops** - list of stopwords and punctuation\n",
    "  * uses first **800** files in each subdirectory for training\n",
    "  * returns data in **X, Y** format\n",
    "  * **X** \n",
    "       > + List of lists (4000 elements in each sublist)\n",
    "       > + each row represents a document\n",
    "       > + each column entry  denotes the frequency of that *feature* in that *document*\n",
    "  * **Y**\n",
    "       > + List   (of 16000 elements)\n",
    "       > + each row represents the category of the corresponding *X row*\n",
    "  * uses **collections.Counter()** to find frequency of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeXY(features, parent_directory, stops):\n",
    "    \n",
    "    dirs = os.listdir(parent_directory)                             # list of subdirectories (=classes)\n",
    "    X = []                                                          # initialise empty lists\n",
    "    Y = []\n",
    "    \n",
    "    for current_dir_index in range(len(dirs)):                      # loop over all subdirectories (20)\n",
    "        \n",
    "        path = '20_newsgroups\\\\' + dirs[current_dir_index]          # path to get inside subdirectory\n",
    "        files = os.listdir(path)                                    # list of files in subdirectory\n",
    "        category = dirs[current_dir_index]                          # subdirectory name = category = class\n",
    "        \n",
    "        for file in files[0:800]:                                   # first 800 files for training\n",
    "            text = []                                             \n",
    "            cnt = Counter()\n",
    "            \n",
    "            path_of_file_in_directory = path + '\\\\' + file          # path to a file\n",
    "            ptr = open(path_of_file_in_directory)                   # returns a pointer\n",
    "            content = ptr.read()                                    # read data from file\n",
    "            \n",
    "            text = word_tokenize(content)                           # clean the data\n",
    "            text = remove_stopwords(text, stops)\n",
    "            \n",
    "            for word in text:                                       # store frequency of each word in file\n",
    "                cnt[word] += 1\n",
    "            \n",
    "            row = [0 for i in range(num_of_features)]               # row of 4000 zeroes\n",
    "\n",
    "            for i in range(num_of_features):                        # for each feature in features \n",
    "                if features[i] in cnt:                              # if the feature in in file\n",
    "                    row[i] = cnt[features[i]]                       # store its frequency at corresponding index\n",
    "\n",
    "            X.append(row)\n",
    "            Y.append(category)\n",
    "        \n",
    "    return X, Y                                                     # lists returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### makeXY_test(features, parent_directory, stops):\n",
    "  > * **features** - array of strings to be used as *features*\n",
    "  > * **parent_directory** - parent directory containing the sub directiories and data files\n",
    "  > * **stops** -> a list of *stopwords* and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeXY_test(features, parent_directory, stops):\n",
    "\n",
    "    dirs = os.listdir(parent_directory)                              # list of subdirectories (=classes)\n",
    "    X = []                                                           # initialise empty lists\n",
    "    Y = []\n",
    "    \n",
    "    for current_dir_index in range(len(dirs)):                       # loop over all subdirectories (20)\n",
    "        \n",
    "        path = '20_newsgroups\\\\' + dirs[current_dir_index]           # path to get inside subdirectory      \n",
    "        files = os.listdir(path)                                     # list of files in subdirectory\n",
    "        category = dirs[current_dir_index]                           # subdirectory name = category = class\n",
    "        \n",
    "        for file in files[800:]:                                     # files after index 800 used for testing-200 files\n",
    "            text = []\n",
    "            cnt = Counter()\n",
    "            \n",
    "            path_of_file_in_directory = path + '\\\\' + file           # path to a file\n",
    "            ptr = open(path_of_file_in_directory)                    # returns a pointer \n",
    "            content = ptr.read()                                     # read data from file\n",
    "            \n",
    "            text = word_tokenize(content)                            # clean the data\n",
    "            text = remove_stopwords(text, stops)\n",
    "            \n",
    "            for word in text:                                        # store frequency of each word in file\n",
    "                cnt[word] += 1\n",
    "            \n",
    "            row = [0 for i in range(num_of_features)]                # row of 4000 zeroes\n",
    "            \n",
    "            for i in range(num_of_features):                         # for each feature in features \n",
    "                if features[i] in cnt:                               # if the feature is in file\n",
    "                    row[i] = cnt[features[i]]                        # store its frequency at corresponding index\n",
    "\n",
    "            X.append(row)\n",
    "            Y.append(category)\n",
    "        \n",
    "    return X, Y                                                      # lists returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_graph(common_words):\n",
    "  > * **common_words** - array of tuples after finding the most_common_words\n",
    "  > * used to find the optimal number of features to choose\n",
    "  > * plots the graphs between word (x-axis) and its frequency (y-axis)\n",
    "  \n",
    "  + **Note** - not used in this project, the number of features best suited were 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(common_words):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for i in common_words:\n",
    "        x.append(i[0])\n",
    "        y.append(i[1])\n",
    "    plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def probability(dictionary, x, current_class):\n",
    "  > * **dictionary** - dictionary built by **fit(X, Y)** \n",
    "  > * **x**  \n",
    " + row in **X_test** whose probability is to be calculated\n",
    " + represents a testing document\n",
    "  > * **current_class** \n",
    " + 1 class out of 20 classes\n",
    " + for which the probability that **x** belongs to **current_class** is calculated\n",
    "  > * *log*  is used to nullify the effect of very small probabilities getting multiplied resulting in even smaller probabilities\n",
    "  > * returns **prob** : the calculated probability \n",
    "  > * **Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    \n",
    "    # documents of current_class / total documents in training set\n",
    "    prob = np.log(dictionary[current_class][\"total_class_docs\"]) - np.log(dictionary[\"total_docs\"])\n",
    "    \n",
    "    for feature_index in range(num_of_features):                     # iterate over all indices: range(4000)\n",
    "        \n",
    "        x_count = x[feature_index]                                   # count of feature[feature_index] in x\n",
    "         \n",
    "        if(x_count!=0):                                              # if the word is present in x\n",
    "            \n",
    "            # laplace correction also done\n",
    "            # count of word in current_class /  total number of words in current_class\n",
    "            num = dictionary[current_class][feature_index] + 1\n",
    "            den = dictionary[current_class][\"total_words\"] + num_of_features\n",
    "            \n",
    "            current_p = np.log(num) - np.log(den)                    # take log\n",
    "            current_p = current_p * x_count                          # multiply with frequency of word in x\n",
    "            prob = prob + current_p                                  # adding log values [log(a*b) = log(a) + log(b)]\n",
    "\n",
    "    return prob                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def predict_single_point(dictionary, x):\n",
    "  > * **dictionary** - dictionary built by **fit(X, Y)**\n",
    "  > * **x** - row in **X_test** whose probability is to be calculated;  represents a testing document\n",
    "  > * out of 20 possible *classes*, finds the **best_class** predicted for **x**, with probability **best_p**\n",
    "  > * returns **best_class** : best class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_point(dictionary, x):\n",
    "    \n",
    "    classes = dictionary.keys()                                       # all classes (20)\n",
    "    best_p = -100000                                                  # initialisation\n",
    "    best_class = ''\n",
    "    first_run = True                                                  # for checking\n",
    "     \n",
    "    for current_class in classes:                                     # loop over all classes\n",
    "        \n",
    "        if current_class == \"total_docs\":\n",
    "            continue\n",
    "        p_current_class = probability(dictionary, x, current_class)   # find prob that x belongs to current_class\n",
    "        \n",
    "        if first_run or p_current_class > best_p:                     # update best_p, if higher prob found\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "            \n",
    "        first_run = False\n",
    "        \n",
    "    return best_class                                                 # best predicted class according to probabilites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def predict(dictionary, X_test):\n",
    "  > * **dictionary** - dictionary built using **fit(X, Y)**\n",
    "  > * **X_test** -\n",
    "  + entire testing data\n",
    "  + numPy array\n",
    "  + *row* represents a document\n",
    "  + *col* represents the feature frequency\n",
    "  > * returns **Y_pred** : list of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):\n",
    "    \n",
    "    Y_pred = []                                                      # initialise empty list\n",
    "    for x in X_test:                                                 # loop over all rows (=documents) in X_test\n",
    "        x_class = predict_single_point(dictionary, x)\n",
    "        Y_pred.append(x_class)                                       # append the predicted class for each x\n",
    "        \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_stopwords(text, stops):\n",
    "  > * removes the *stopwords* from a given **text**\n",
    "  > * **text** - data *list* from which *stopwords* are to be removed\n",
    "  > * **stops** - list of all stopwords and punctuation marks\n",
    "  > * returns **clean_text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stops):\n",
    "    clean_text = [w for w in text if w not in stops]         # contains words which are not in stopwords\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "\n",
    "# Main code\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > reading and cleaning the entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7110266\n"
     ]
    }
   ],
   "source": [
    "parent_directory = '20_newsgroups'                           # contains all subdirectories and data files\n",
    "text = get_data(parent_directory)                            # array of words(from all data files)\n",
    "\n",
    "print(len(text))                                             # verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = get_stopwords()                                      # list of stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3830370\n"
     ]
    }
   ],
   "source": [
    "clean_text = remove_stopwords(text, stops)                   # array of words with stopwords removed\n",
    "\n",
    "print(len(clean_text))                                       # verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text                                            # copy the cleaned data back to \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []                                              # initialize a list for storing lemmatized words \n",
    "clean_text = lemmatize_text(text)                            # returns the lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3555248"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_text)                                              # length reduces further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> extracting the top 4000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = get_common_words(num_of_features, clean_text)              # 4000 most common words needed\n",
    "# tuple returned - most_common_words is a tuple (word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--', \"''\", '``', \"'s\", \"n't\", 'cantaloupe.srv.cs.cmu.edu', 'subject', 'date', 'apr', 'newsgroups', 'path', 'organization', 'message-id', 'lines', '...', 'gmt', 'would', 'one', '1993', 'writes', 'use', 'get', 'article', 'say', 'references', 'news', 'know', 'sender', 'howland.reston.ans.net', 'people', 'like', 'make', '1', 'go', 'university', 'x', 'well', 'think', 'nntp-posting-host', '2', '93', 'time', 'zaphod.mps.ohio-state.edu', 'see', 'also', 'system', 'could', 'crabapple.srv.cs.cmu.edu', 'new', 'work', 'take', 'may', \"'m\", 'good', 'year', 'want', 'right', 'noc.near.net', '0', 'news.sei.cmu.edu', 'need', 'even', 'das-news.harvard.edu', 'way', 'xref', 'europa.eng.gtefsd.com', 'thing', 'come', 'fs7.ece.cmu.edu', 'look', 'give', 'usenet', 'problem', '3', 'god', 'uunet', 'many', 'much', 'two', 'first', 'file', 'u', 'try', 'distribution', '5', '4', '6', 'world', 'question', 'call', 'cis.ohio-state.edu', 'state', 'point', 'believe', 'anyone', \"'ve\", 'ca', '20', 'program', 'post', 'run', 'magnesium.club.cc.cmu.edu', 'write', 'seem', 'computer', 'something', 'rochester', '16', 'help', 'number', 'christian', 'really', 'please', 'include', \"'re\", 'drive', 'since', 'gatech', 'mean', '15', 'still', 'back', 'find', 'day', 'government', 'start', 'case', '21', 'game', 'information', 'law', 'part', 'netcom.com', 'cs.utexas.edu', 'might', 'fact', 'last', 'must', 'support', 'read', 'never', 'reply-to', '8', 'group', 'thanks', \"'ll\", \"'d\", 'udel', 'let', 'key', 'without', 'car', 'version', 'david', 'sure', 'another', 'set', 'follow', 'usa', 'reason', 'someone', '7', 'tell', 'data', 'line', 'ask', 'put', 'available', 'name', 'space', 'power', 'lot', 'change', 'great', 'show', '10', 'science', 'place', 'window', 'software', 'life', 'high', 'list', 'card', 'anything', 'c', 'opinion', 'bad', 'little', 'etc', 'word', 'however', 'tue', '19', 'john', 'long', 'bb3.andrew.cmu.edu', 'around', 'image', 'gun', 'every', 'different', 'course', '23', 'true', 'team', 'book', 'probably', 'keep', 'best', 'us', 'usc', 'darwin.sura.net', 'least', '9', 'kill', 'bit', 'order', 'public', 'fri', 'consider', 'chip', 'user', 'old', 'second', 'actually', 'enough', 'talk', '17', 'idea', '22', 'message', 'control', 'end', 'claim', 'jesus', 'base', 'child', 'example', 'note', 'though', 'q', 'next', 'provide', 'happen', 'either', 'research', 'issue', 'possible', 'answer', 'far', 'send', 'nothing', 'source', '18', 'else', 'windows', 'real', 'mail', 'b', 'rather', 'person', 'found', 'athos.rutgers.edu', '24', 'cause', 'hard', 'thought', 'center', 'value', 'technology', 'big', 'sun', 'wrong', 'do', 'p', 'free', 'thu', 'play', 'bill', 'internet', 'others', 'human', 'yes', '25', 'man', 'price', 'disk', 'uwm.edu', 'email', 'maybe', 'able', 'standard', 'r', 'access', 'phone', 'israel', 'machine', 'mark', 'address', 'large', 'inc.', 'kind', 'turn', 'mon', 'view', 'v', 'n', 'yet', '14', 'ever', 'require', 'e', 'hand', 'usenet.ins.cwru.edu', 'understand', 'national', 'always', 'evidence', 'general', 'result', 'quite', 'less', 'driver', 'news.cso.uiuc.edu', 'heard', 'area', 'feel', 'three', 'code', 'whether', 'type', 'small', 'fire', '12', 'wed', 'report', 'jews', 'sell', 'remember', 'left', 'home', 'several', 'company', 'live', 'away', 'keywords', 'interest', 'today', 'info', 'win', 'test', 'force', 'e-mail', 'talk.politics.misc', 'matter', 'sound', 'pay', 'country', 'talk.religion.misc', 'institute', 'copy', 'mac', 'agree', 'allow', 'w', 'player', 'l', '30', 'alt.atheism', 'application', '11', 'g', 'sale', 'speed', 'move', 'open', 'michael', 'cost', 'geneva.rutgers.edu', 'care', 'add', 'net', 'box', 'frank', 'week', 'j', '13', 'become', 'argument', 'perhaps', 'stop', 'side', 'department', 'religion', 'agate', 'steve', 'network', 'already', 'local', 'service', 'create', 'american', 'wupost', 'war', 'server', 'money', 'followup-to', 'love', 'color', 'guy', 'design', 'study', 'mr.', 'low', 'church', 'president', 'exist', 'difference', 'deal', 'stuff', 'current', 'h', 'hope', 'mention', 'mind', 'interested', 'bogus.sura.net', 'buy', 'ago', 'whole', 'offer', 'paul', 'jim', 'sdd.hp.com', 'expect', 'mike', 'told', 'lead', 'package', 'assume', 'form', 'bible', 'woman', 'member', 'check', 'fax', 'pretty', 'death', 'appear', 'f', 'month', 'encryption', 'house', '27', 'everyone', 'school', '26', 'pc', 'display', 'statement', 'model', 'original', 'rule', 'action', 'effect', 'truth', 'begin', 'save', 'dos', 'everything', 'robert', 'april', 'format', 'memory', 'die', 'simply', 'city', 'contact', 'easy', 'k', 'attack', 'often', 'guess', 'important', 'full', 'talk.politics.guns', 'sort', 'wonder', 'error', 'canada', 'reference', 'na', 'college', 'light', 'ftp', 'black', 'friend', 'level', 'fbi', 'clipper', 'almost', 'caen', 'body', 'carry', 'head', 'return', 'position', 'rutgers', 'certainly', 'engineering', 'approved', 'apple', 'hit', 'magnus.acs.ohio-state.edu', 'earth', 'experience', 'instead', 'video', 'sci.crypt', 'appreciate', 'health', 'moral', 'systems', 'history', 'wo', 'board', 'oh', 'unix', 'suppose', 'act', 'theory', 'sci.space', 'situation', 'device', 'present', 'monitor', 'although', 'event', 'later', 'close', 'job', 'rec.autos', 'arm', 'lose', 'comp.graphics', 'turkish', 'scsi', 'sci.med', 'white', '28', 'discussion', 'hold', 'comp.sys.mac.hardware', 'rec.motorcycles', 'drug', 'sense', 'couple', 'comment', 'bike', 'california', 'private', 'comp.sys.ibm.pc.hardware', 'belief', 'hell', 'site', 'comp.windows.x', 'press', 'rate', 'jewish', 'sci.electronics', 'men', 'morality', 'term', 'anyway', 'police', 'explain', 'faith', 'rec.sport.hockey', 'talk.politics.mideast', 'within', 'society', 'x-newsreader', 'dept', 'clear', 'correct', 'unless', 'major', 'misc.forsale', 'single', 'comp.os.ms-windows.misc', 'weapon', 'cover', 'christians', 'andrew.cmu.edu', 'product', 'text', 'summary', 'objective', 'page', 'individual', 'similar', 'size', 'christ', 'israeli', '1.1', 'land', 'anybody', 'ground', 'rec.sport.baseball', 'political', 'account', 'especially', 'speak', 'soc.religion.christian', 'project', 'igor.rutgers.edu', 'face', 'brian', 'release', 'fast', 'receive', 'likely', 'sat', 'via', 'hedrick', 'saw', 'plan', 'armenian', 'involve', 'suggest', 'define', 'learn', 'nice', 'aramis.rutgers.edu', 'attempt', 'function', 'clinton', '*/', 'business', 'quote', 'object', 'figure', 'certain', 'wait', 'response', 'ibm', 'party', 'division', 'request', 'top', 'reply', '29', 'night', 'san', 'period', 'regard', '..', 'simple', 'except', 'mode', 'security', 'james', 'bob', 'purpose', 'fine', 'watch', 'swrinde', 'record', 'decide', 'emory', 'road', 'exactly', 'religious', 'screen', 'policy', '50', 'dead', 'crime', 'fan', 'manager', 'article-i.d', 'ogicse', 'tin', 'usually', 'million', 'among', 'story', 'common', 'sorry', 'thus', 'rest', 'mine', 'reading', 'owner', 'hockey', 'sol.ctr.columbia.edu', 'season', 'output', '/*', 'york', 'dave', 'third', 'upon', 'office', 'per', 'process', 'leave', 'accept', 'whatever', 'early', 'command', 'particular', 'u.s.', 'armenians', 'faq', 'increase', 'stand', 'protect', '100', 'advance', 'graphic', 'tax', 'young', 'koresh', 'hear', 'field', 'strong', 'hardware', 'atheist', 'z', 'corporation', 'medical', 'therefore', 'western', 'c.', 'washington', 'four', 'pacific.mps.ohio-state.edu', 'building', 'express', 'pitt.edu', 'late', '32', 'shot', 'hi', 'personal', 'method', 'elroy.jpl.nasa.gov', 'special', 'condition', 'freedom', 'short', 'radio', 'cut', 'past', 'goal', 'insurance', 'bus', 'peace', 'launch', 'due', 'ok', 'food', 'keith', 'peter', 'federal', 'tape', 'port', 'specific', 'j.', 'gas', 'future', 'total', 'andrew', 'db', 'delete', 'pittsburgh', 'news-feed-1.peachnet.edu', 'dod', 'sometimes', 'texas', 'produce', 'murder', 'longer', 'directory', 'poster', 'sin', 'section', 'algorithm', 'detail', 'ames', 'font', 'nasa', 'break', 'notice', 'paper', 'authority', 'baseball', 'lie', 'league', 'recently', 'doubt', 'front', 'ii', 'fall', '40', 'modem', 'worth', 'entry', 'plus', 'outside', 'miss', 'eric', 'handle', 'tom', 'toronto', 'choice', 'manual', 'smith', 'widget', 'united', 'series', 'hour', 'family', 'court', 'letter', 'compare', 'complete', 'military', 'a.', 'engine', 'chance', 'water', 'option', 'red', 'international', 'library', 'wish', 'horus.ap.mchp.sni.de', '-+', 'apply', 'prove', 'switch', 'states', 'anonymous', 'build', 'al', 'continue', 'necessary', 'behind', 'various', 'disclaimer', 'ram', 'near', 'author', 'resource', 'jon', 'activity', '**', 'pipex', 'none', 'tool', 'practice', 'score', 'richard', 'along', 'mouse', 'thomas', 'amount', '_/', 'previous', 'voice', 'draw', 'class', 'fix', 'scott', 'community', 'agency', 'performance', 'controller', 'together', 'm.', 'document', 'soon', 'charge', 'abortion', 'fail', 'communication', 'refer', 'legal', 'haven.umd.edu', 'minute', 'services', 'feature', 'd.', 'official', 'interface', 'development', 'shall', 'administration', 'language', '31', 'citizen', 'bbs', 'unit', 'population', 'technical', '1992', 'sent', 'door', '-0400', 'share', 'quality', 'tv', 'willing', 'not-for-mail', 'pat', 'approach', 'definition', 'age', 'p.', 'station', 'christianity', 'solution', 'gap.caltech.edu', 'currently', 'chris', 'king', 'secret', 'hole', 'knowledge', 'vote', 'ray', 'chicago', 'mile', 'prevent', 'supply', 'search', 'otherwise', 'pick', 'average', 'remove', 'criminal', 'load', 'spool.mu.edu', 'george', 'newsgroup', 'basic', '33', 'cable', 'america', 'uknet', 'convert', 'decision', 'doctor', 'replace', 'trade', 'useful', 'ux1.cso.uiuc.edu', 'nature', 'final', 'disease', 'picture', 'effort', 'clearly', 'exists', 'boston', 'air', 'universe', 'eye', 'ed', 'motif', 'network.ucsd.edu', 'de', 'access.digex.net', 'student', 'scientific', 'flame', 'avoid', 'print', 'printer', 'north', 'illinois', 'mass', 'existence', 'main', 'living', 'father', 'ide', 'serial', 'serve', 'patient', 'bring', 'fight', '35', 'market', 'batf', 'room', 'slow', 'joe', 'respond', 'ny', 'henry', 'basis', 'drop', 'operation', 'built', 'necessarily', 'serious', 'reach', 'meaning', 'decwrl', 'inside', 'agent', 'indeed', 'cdt', 'block', 'noise', 'instal', 'cleveland.freenet.edu', 'sign', 'privacy', 'graphics', 'completely', 'finally', 'computing', 'realize', 'dr.', 'street', 'e.', 'mcsun', 'range', 'bought', 'uk', 'commercial', 'normal', 'torn', 'obviously', 'dan', 'son', 'connect', 'defense', 'faster', 'gary', 'reasonable', 'medium', 'r.', 'digital', 'determine', 'contain', 'volume', 'lack', 'arab', '2000', 'raise', 'neither', 'cheap', 'univ', '||', 'la', 'germany', 'concern', 'thank', 'input', 'half', 'suggestion', 'directly', 'intend', 'bnr.ca', 'argue', 'obtain', 'material', 's.', 'accord', 'easily', 'reality', 'respect', 'step', 'laboratory', 'world.std.com', 'armenia', 'purchase', 'commit', 'kid', 'title', 'lord', 'entire', 'store', 'pass', 'measure', 'suspect', 'stay', 'genocide', 'nhl', 'imagine', 'analysis', 'suffer', 'in-reply-to', 'signal', 'happy', 'trouble', 'advice', 'st.', 'tim', 'heart', 'fit', 'livesey', 'cramer', 'l.', 'limited', 'hp', '***', '3.1', 'homosexual', 'folk', 'trust', 'floppy', 'related', 'cornell', 'turkey', 'evil', '34', 'keyboard', 'difficult', 'po.cwru.edu', 'grant', 'prefer', 'environment', 'tank', 'choose', 'alone', 'mission', 'lab', 'moon', 'i.e', 'matthew', 'judge', 'joseph', 'des', 'direct', 'count', 'acsu.buffalo.edu', 'microsoft', 'lee', 'forget', 'batcomputer', '80', 'illegal', 'amendment', 'south', 'oil', 'unfortunately', 'happens', 'ken', 'thousand', 'safety', 'waco', 'optilink.com', 'archive', 'risk', 'shoot', 'purdue', 'wife', 'secure', 'treatment', 'firearm', 'listen', 'clock', 'responsible', 'update', 'west', 'conference', 'orbit', 'whose', '60', 'pitch', 'dealer', 'nation', 'generally', 'jack', 'degree', 'transfer', 'satellite', 'ability', 'sex', 'client', 'ten', 'addition', 'recent', 'leader', 'fund', 'coverage', 'apparently', 'gay', 'context', 'compound', 'east', 'appropriate', 'virginia.edu', 'publish', 'gordon', 'blue', 'occur', 'stephen', 'internal', 'limit', 'civil', 'brought', 'greek', 'stupid', 'saimiri.primate.wisc.edu', 'enforcement', 'remain', 'edt', 'admit', 'eat', 'nobody', 'mother', 'dog', 'pull', 'adam', 'originator', 'w.', 'held', 'walk', 'russian', 'utnut', 'throw', 'reduce', 'wire', 'obvious', 'vehicle', 'character', 'concept', 'mnemosyne.cs.du.edu', 'online', 'equipment', 'gm', 'congress', 'ms', 'physical', 'knew', 'extra', 'recognize', 'psinntp', 'btw', 'heaven', 'battery', '0d', 'reserve', 'village', 'conclusion', 'doug', 'officer', 'contains', 'push', 'represent', 'bear', 'treat', 'effective', 'march', 'mormons', 'muslim', 'turks', 'somebody', 'colorado', 'education', 'georgia', 'proof', 'planet', 'across', 'committee', 'blood', 'pin', 'larry', 'ride', 'cpu', 'midway.uchicago.edu', '36', '45', 'green', 'playoff', 'surface', 'race', 'soldier', 'gateway', 'excellent', 'absolute', 'constitution', 'os/2', 'foot', 'alternative', '38', 'van', 'license', 'news.udel.edu', 'connection', 'upgrade', 'property', 'thread', '___', 'roger', 'magazine', 'basically', 'communications', 'los', 'trial', 'news.columbia.edu', 'false', 'warrant', 'mantis.co.uk', 'fido.asd.sgi.com', 'electronic', 'yeah', 'enter', 'ron', 'star', 'somewhere', 'establish', 'protection', 'rid', 'fear', 'pas', 'msg', 'beyond', 'logic', 'escape', 'administrator', 'damage', 'review', 'talk.abortion', 'americans', 'middle', 'five', 'army', 'ignore', 'social', 'natural', 'link', 'murdoch.acc.virginia.edu', 'perform', 'majority', 'bu.edu', 'tire', 'select', 'associate', 'propose', 'newsserver.jvnc.net', 'responsibility', 'linac', 'sit', 'cs', 'phil', 'ad', 'item', 'recall', 'modern', 'particularly', 'multiple', 'circuit', 'branch', 'jeff', 'loss', '37', '3d', 'demand', '42', 'separate', 'byte', '1991', 'table', 'round', 'possibly', 'virginia', 'cd', 'shipping', 'century', 'seriously', 'medicine', 'played', 'civilian', 'mistake', 'tel', 'piece', 'former', 'netnews.upenn.edu', 'ball', 'rob', 'park', 'austin', 'personally', 'develop', 'leland.stanford.edu', 'earlier', 'transfer.stratus.com', 'cs.cmu.edu', \"o'dwyer\", 'poor', 'neighbor', 'dangerous', 'independent', 'perfect', 'nt', 'shell', 'moment', 'hate', 'pain', 'md', 'nsa', 'stick', 'topic', 'fairly', 'mhz', 'kept', 'cambridge', 'brain', 'brother', 'mr', 'nuclear', 'australia', 'ucsu.colorado.edu', 'atheism', 'lock', 'mary', 'maintain', 'csus.edu', 'possibility', 'factor', 'andy', '__', 'meg', 'stephanopoulos', 'morning', 'energy', 'scripture', 'ohio', 'parent', 'sgiblab', 'generate', 'straight', 'boulder', 'charles', 'destroyer', 'remote', 'oppose', 'geb', 'and/or', 'debate', 'double', 'hot', 'hey', 'solntze.wpd.sgi.com', 'forward', 'hall', 'william', 'beat', '1990', 'id', 'totally', 'serdar', 'argic', 'billion', 'kevin', 'angeles', 'absolutely', 'recommend', 'burn', 'regular', 'cobb', 'behavior', '1st', 'justice', 'frame', 'instance', 'hundred', 'rock', 'greatly', 'register', 'mathew', 'waste', 'sgi', 'hello', 'description', 'popular', 'flight', 'fair', 'umn.edu', 'bother', 'convention', 'merely', 'advantage', 'proper', 'vga', 'bd', 'hospital', 'refuse', 'teach', 'traffic', 'utility', 'dollar', 'repeat', 'uucp', 'proposal', 'escrow', 'minority', 'master', 'vs.', 'sample', 'agate.berkeley.edu', 'allows', 'netnews', 'enterpoop.mit.edu', 'atf', 'mb', 'described', 'usenet.ucs.indiana.edu', 'mentor.cc.purdue.edu', 'att', 'safe', 'expensive', 'british', 'percent', 'greg', 'bell', 'mountain', 'bmw', '41', 'cancer', 'meant', 'cup', 'art', 'paid', 'defend', 'sexual', 'discuss', 'meet', 'europe', 'zoo.toronto.edu', 'deny', 'resolution', 'developed', 'bunch', 'concert', 'inc', 'nntp', 'cult', 'cop', 'holy', 'fill', 'actual', 'h.', 'cross', 'alan', 'rocket', 'aware', 'convince', 'town', 'conflict', 'postscript', 'shuttle', 'jake', 'specifically', 'posting', 'welcome', 'spirit', 'crypto', 'uxa.cso.uiuc.edu', 'daniel', 'impossible', 'soul', 'wall', 'processing', 'benefit', 'quick', 'islamic', 'nj', '408', 'jason', 'tend', 'equal', 'motorcycle', '2nd', 'assumption', 'weight', 'implement', 'mostly', 'motherboard', 'entirely', 'imply', 'reader', 'interpretation', 'brad', 'six', 'militia', 'serum', 'animal', 'claimed', 'tony', 'root', 'solar', 'cleveland', 'radar', 'observation', 'concerned', 'disagree', 'penalty', 'improve', 'manage', 'training', 'news.intercon.com', 'ub', '200', 'confuse', 'banks', 'gain', 'innocent', 'additional', 'iastate.edu', 'abuse', 'besides', 'target', 'creation', 'rich', 'zuma.uucp', 'pp', 'meeting', 'host', 'pressure', 'highly', 'sky', 'industry', 'sport', 'corp.', 'bug', 'towards', 'islam', 'wear', 'g.', 'club', 'bruce', 'wide', 'soviet', 'discus', 'significant', 'buying', 'bank', 'join', 'sandvik', '//', 'routine', 'encrypt', 'scientist', 'observe', 'status', 'pa', 'shift', '39', 'everybody', 'motorola', 'dc', '90', 'fully', 'blame', 'b.', 'homosexuality', 'simms', 'hurt', 'ticket', 'telephone', 'math', '300', 'bios', 'movement', 'news.service.uci.edu', 'track', 'install', 'cache', 'detector', 'psuvm.psu.edu', 'crash', '==', 'structure', 'screw', 'fun', 'external', 'finish', 'minor', 'wiretap', 'mit', 'born', 'desire', 'role', 'indicate', 'wave', 'edge', 'athena.mit.edu', 'default', 'match', 'discover', 'ryan', 'sandvik-kent.apple.com', '70', 'spend', 'assault', 'jpeg', 'dscomsa.desy.de', 'random', 'valid', 'olivea', 'damn', 'heat', 'os', 'promise', 'guide', 'morris', '51', 'news.duke.edu', 'nigel.msen.com', 'depend', 'co', 'ted', 'editor', 'worship', 'attitude', 'eternal', 'typical', 'apollo.hp.com', 'tie', 'ie', 'steal', 'xterm', 'igc.apc.org', 'evolution', 'accepted', 'worry', 'expert', 'background', '43', 'employer', 'european', 'experiment', 'opportunity', 'facility', 'button', 'spacecraft', 'content', 'management', 'electronics', '1988', 'attention', 'principle', 'generation', 'liberty', 'immediately', 'et', 'kent', 'halat', 'guest', 'rick', 'handgun', '55.0', 'pgp', 'luck', 'potential', 'cool', 'soc.culture.turkish', 'gif', 'detroit', 'vancouver', 'technique', 'patch', 'pixel', 'plug', 'e.g', 'engineer', '3rd', 'marc', 'reject', 'scheme', 'daily', 'session', 'seattle', 'probe', 'properly', 'verse', 'english', 'terrorist', 'warn', 'manufacturer', 'boy', 'terminal', 'despite', 'cs.ubc.ca', 'isa', 'victim', 'alive', 'direction', 'announce', 'steven', 'tx', 'doctrine', 'sunday', 'craig', 'yesterday', 'lunar', 'german', 'news-software', 'bar', 'domain', 'd012s658.uucp', 'massacre', 'cunixb.cc.columbia.edu', 'enjoy', 'davidians', 'draft', 'pro', 'tech', 'capability', 'tear', 'bet', 'documentation', 'export', 'brake', 'describe', 'tm', '48', 'reaction', 'ice', 'component', 'location', 'macintosh', 'cs.pitt.edu', 'arabs', 'spent', 'conversation', 'libertarian', 'channel', 'nearly', 'association', 'excuse', 'somehow', 'accurate', 'munnari.oz.au', 'border', 'wayne', 'montreal', 'freenet.carleton.ca', 'x11r5', 'soc.culture.jewish', 'troop', 'backup', 'eventually', '44', 'plane', 'fly', 'witness', 'focus', 'ahead', 'instruction', 'ready', 'boot', 'convex', 'aspect', 'ira.uka.de', 'becomes', 'adobe.com', 'element', 'electrical', 'miller', 'catholic', 'ensure', 'ontario', 'aids', 'male', 'summer', 'automatic', 'distance', 'procedure', 'eff', 'biblical', 'allen', 'vax/vms', 'self', 'access.digex.com', 'clayton', 'panix.com', 'braves', 'somewhat', 'conduct', 'muslims', 'okay', 'requirement', 'distribute', 'compile', 'rom', 'hd', 'translation', 'intelligence', 'spot', 'foreign', 'bos', 'xt', 'espn', 'quickly', 'martin', 'tradition', 'threat', 'modify', 'audio', 'platform', 'sci.astro', 'louis', 'passage', 'expansion', 'pitcher', 'explanation', 'douglas', 'database', 'bony1.bony.com', 'comp.org.eff.talk', 'positive', 'challenge', 'economic', 'digex.com', 'urbana', 'quadra', '3.0', 'anywhere', 'judgement', 'pl8', 'francisco', 'style', 'heavy', 'visit', 'complex', 'announcement', 'seek', 'jose', 'prediction', 'dream', 'relationship', 'cup.portal.com', 'huge', 'specify', 'sick', 'central', 'definitely', 'favor', 'senator-bedfellow.mit.edu', 'afraid', 'stats', 'according', 'identify', 'cold', 'exercise', 'setup', 'visual', 'fuel', 'parallel', 'johnson', 'florida', 'michigan', 'gld', 'austin.ibm.com', 'exact', 'ban', 'jew', 'perry', 'grow', 'speech', 'string', 'schedule', 'slot', 'configuration', 'rider', 'spread', 'length', '-0500', 'historical', 'southern', 'brown', 'ford', 'int', 'budget', 'origin', 'clean', 'mellon', 'baby', 'frequently', 'fault', 'scale', 'cal', 'foundation', 'processor', 'sciences', 'permit', 'toward', 'spec', 'ran', 'ring', 'seat', 'p.o', 'slightly', 'accident', 'blue.cis.pitt.edu', 'rise', 'knock', 'ship', 'deserve', 'candidate', 'correctly', 'gotten', 'mount', 'shape', 'ati', 'hang', 'carnegie', 'solve', 'solid', 'comparison', 'filter', 'virtual', 'normally', 'holocaust', 'misc.legal', 'music', 'prayer', 'marriage', 'territory', 'colour', 'maynard', 'fred', 'buf', 'weak', 'mechanism', 'cryptography', 'uicvm.uic.edu', 'buffalo', 'pray', '47', 'advanced', 'relative', 'matt', 'survey', 'patrick', 'compatible', 'stream', 'netcom', 'relevant', 'workstation', 'percentage', 'atlanta', 'carl', 'amiga', 'cheaper', 'randy', 'culture', 'brand', 'onto', 'detect', 'hearing', 'yuma.acns.colostate.edu', 'kelvin.jpl.nasa.gov', 'rangers', 'professor', 'legitimate', 'wheel', 'news.u.washington.edu', '500', 'floor', 'alchemy.chem.utoronto.ca', 'depends', 'felt', 'diego', 'apartment', 'island', 'prism.gatech.edu', 'leafs', 'afford', 'render', 'usual', 'laugh', 'nazi', 'questions', '256', 'binary', 'installation', 'implementation', 'vnews', 'capable', 'june', '2.0', 'russia', 'wings', 'plain', 'connector', 'journal', 'active', 'astronomy', 'nott', 'map', 'polygon', 'aid', 'initial', 'contract', 'imho', 'bush', 'stone', 'nec', 'dale', 'ottawa', 'admin', '46', 'temperature', 'incident', 'ramsey.cs.laurentian.ca', 'jumper', 'truly', 'success', 'geraldo.cc.utexas.edu', 'etc.', 'fee', 'diet', 'injury', 'professional', 'auto', 'watson.ibm.com', 'f.', '55', 'plot', 'reveal', 'camera', 'twice', 'ns1.cc.lehigh.edu', 'xv', 'menu', 'spencer', 'openwindows', 'k.', 'sentence', 'sequence', 'chemistry', 'wam.umd.edu', 'moe.ksu.ksu.edu', 'infection', 'broken', 'philadelphia', 'impact', 'strange', 'extend', 'punishment', 'noose.ecn.purdue.edu', 'girl', '2.5', 'failure', 'fool', 'zone', 'panel', 'demonstrate', 'curious', 'denver', 'news.iastate.edu', 'participate', 'japanese', 'strike', 'lewis', 'slave', 'helmet', 'urartu.sdpa.org', 'own', '150', 'frequency', 'buffer', 'attorney', 'encourage', 'bias', 'edit', 'director', 'cite', 'hardly', 'sleep', 'centre', 'violence', '75', 'minnesota', 'max', 'extension', 'mars', 'honda', 'cco.caltech.edu', 'talk.origins', 'cell', 'affect', 'regardless', '49', 'movie', 'strip', 'swap', 'coach', 'destroy', 'surprised', 'davis', 'dec', 'echo', 'funny', 'task', 'iii', 'anymore', 'ncsu.edu', '1989', 'il', '250', '65', 'canadian', 'truck', 'copyright', 'alt.conspiracy', '59', 'edward', 'uvaarpa', 'touch', 'amp', 'surely', 'assertion', 'sea', 'transmission', 'lawrence', 'survive', 'academic', 'salt', 'irvine', 'mask', 'aurora.alaska.edu', 'guilty', 'cycle', 'dare', 'region', '+1', 'organize', 'think.com', 'dark', 'stanford', 'spring', 'complain', 'gear', 'times', 'palestinian', '1/2', 'csd-newshost.stanford.edu', 'interpret', 'suit', 'primary', 'plant', 'statistic', 'credit', 'locate', 'bay', 'programmer', 'houston', 'wind', 'edition', 'acceptable', 'capital', 'manner', 'powerful', 'prior', 'crack', 'travel', 'band', 'wanted', 'est', 'nick', 'london', 'essentially', 'violation', 'corner', 'deep', 'logical', 'extremely', '145', 'walker', 'x11', 'columbia', 'ancient', 'gene', 'caught', 'stage', 'decent', 'newspaper', 'speaker', 'roy', 'operating', 'registration', 'england', 'equivalent', 'council', 'wood', 'proceed', 'apple.com', 'guarantee', 'follower', 'physics', 'expose', 'adams', 'jonathan', 'jerusalem', 'kit', 'hallam', 'candida', 'met', 'nyx.cs.du.edu', 'protocol', 'mirror', 'compromise', 'perfectly', 'sgigate', 'simon', 'hop', 'sfu.ca', '486', \"'93\", 'spot.colorado.edu', 'pit', 'laser', 'berkeley', 'mouth', 'bottom', 'feed', 'char', '53', 'lake', 'lay', 'diamond', 'cs.uiuc.edu', 'originally', 'circumstance', 'finger', 'writer', 'collection', 'rat', 'vendor', 'neutral', 'reflect', '54', 'strongly', 'leaf', 'organpipe.uug.arizona.edu', 'jet', 'adapter', 'lebanon', 'conclude', 'contrary', 'va', 'suicide', 'secretary', 'motor', 'azerbaijan', 'newsletter', 'variety', 'forever', 'prophecy', 'reliable', 'v.', 'clark', 'dog.ee.lbl.gov', 'accelerator', 'burst', 'achieve', 'yale', 'packet', 'familiar', '1.41', 'consideration', 'maryland', '400', 'submit', 'resistance', 'gravity', '52', 'vision', 'shop', 'flyers', 'sternlight', 'kingdom', 'sufficient', 'apart', 'sw.stratus.com', 'train', 'relatively', 'trip', 'japan', 'compression', 'contest', 'cpr', 'baltimore', 'malgudi.oar.net', 'tube', 'paint', '61', 'comp', 'eisa', 'venus', 'phill', 'santa', 'fundamental', 'consistent', 'vice.ico.tek.com', 'condemn', 'bomb', 'split', 'predict', 'hint', 'microsoft.com', 'stanley', 'toolkit', 'sci', 'saturn', 'xlib', 'defensive', 'rear', 'shout', 'danger', 'aside', 'remark', 'hill', 'zero', 'raid', 'scan', 'latter', 'ultb.isc.rit.edu', 'satan', 't.', 'arrive', 'anderson', 'centris', 'tor', 'previously', 'exchange', 'violent', 'sunic', 'que', 'circle', 'revelation', 'pattern', 'liberal', 'variable', 'myers', 'desktop', 'pasadena', 'notion', 'phrase', 'examine', '*not*', 'kick', 'palestinians', 'joke', 'dean', 'attach', 'det', 'surprise', 'regards', 'ai', 'investigation', 'shelley.u.washington.edu', 'balance', 'surrender', 'gaza', 'collect', 'christopher', '****', 'rtsg.mot.com', 'optilink', 'importance', 'enemy', 'drink', 'prophet', 'symptom', 'cry', 'insert', 'seems', 'wound', 'rush', 'null', 'intel', 'arrest', 'converter', 'voltage', 'israelis', 'contribute', 'ignorance', 'careful', 'newton.apple.com', 'overall', 'testament', 'paragraph', 'chapter', 'suck', 'w/', 'sector', 'clutch', 'usage', 'tree', 'chemical', 'crowd', 'st', 'annual', 'galileo.cc.rochester.edu', 'funding', 'approve', 'proven', 'mi', 'pair', 'karl', 'agreement', 'visible', 'pm', 'mksol.dseg.ti.com', 'outlet', 'lebanese', 'spending', 'advocate', 'duty', 'exception', 'given', 'senate', 'indiana', 'occupy', 'friday', 'maximum', 'trace', 'hence', 'birth', 'restriction', 'finland', 'printing', 'flow', 'murray', 'vela.acs.oakland.edu', 'ozonehole.com', 'utkvm1.utk.edu', 'presence', \"i'm\", '56', '64', 'rights', 'shut', 'lady', 'animation', 'shareware', 'baalke', 'dennis', 'chi', 'restrict', 'estimate', 'attribute', 'sad', 'instrument', 'career', 'wilson', 'tower', 'engage', 'violate', 'customer', 'jerry', 'checked', 'iran', 'sun-barr', 'capture', '85', 'login', 'index', 'hitter', 'palestine', 'rely', 'terrorism', 'irrelevant', 'category', 'shit', 'winner', 'seven', 'honest', 'accuracy', 'compress', 'kenneth', 'xview', 'cs.cornell.edu', 'puck', 'unknown', 'closer', 'hitler', 'progress', 'occurs', 'using', 'russell', 'remains', 'daughter', 'offense', 'repair', 'sci.skeptic', 'propulsion', 'orion.oac.uci.edu', 'hst', 'dyer', 'justify', 'silly', 'portion', 'drag', '76', 'hypothesis', 'sony', 'bitmap', '800', 'portable', 'staff', '-1', 'newsflash.concordia.ca', 'winnipeg', 'blues', 'harm', 'yale.edu', 'graham', 'cheers', 'netcomsv', 'wisdom', 'seal', 'holland', 'doc.ic.ac.uk', 'icon', 'ccu.umanitoba.ca', 'dsd.es.com', 'negative', 'dictionary', 'replacement', 'arromdee', 'p.s', 'loop', 'taught', 'warning', 'throughout', 'pope', 'applies', 'tiff', 'stereo', 'divine', 'successful', 'gospel', '3.5', 'francis', 'lawyer', 'pointer', 'nc', 'ncar', '_____', 'production', 'gerald', 'core', 'assert', 'conversion', 'unlikely', 'carefully', '\\\\/', 'pop', 'blow', 'senior', '386', 'cd-rom', 'hydra.gatech.edu', 'rsa', 'davidian', 'equally', 'dbstu1.rz.tu-bs.de', 'combination', 'planning', 'subjective', 'saturn.wwc.edu', 'stock', 'jones', 'ps', 'fpu', 'harry', 'phenomenon', 'introduce', 'fellow', '66', 'survivor', 'atmosphere', '1000', 'expand', 'automatically', 'battle', 'destroyed', 'c++', 'hudson', 'scsi-2', 'cipher', 'depth', 'blaze.cs.jhu.edu', 'plate', 'wild', 'kyle.eitech.com', 'award', 'combine', 'inch', 'well.sf.ca.us', 'turbo', 'module', 'carter', 'transmit', 'microsystems', 'canon', 'sheet', 'observer', 'label', 'influence', 'literature', 'threaten', 'pad', 'amateur', 'hewlett-packard', 'exit', 'min', 'duo', 'philosophy', 'execute', 'continued', 'intent', 'glad', 'representative', 'math.fu-berlin.de', 'jersey', 'cat', 'global', 'header', 'massachusetts', 'greece', 'unable', 'supposedly', 'log', 'roll', 'french', 'pdt', '-0700', 'grace', 'ab', 'harvard', '82', 'chuck', 'yeast', 'prepared', 'immoral', 'firm', 'pl9', 'u.', 'complaint', 'burning', 'studies', 'trick', 'pitt', 'n.', 'lucky', 'sweden', 'storage', 'lc', 'silver.ucs.indiana.edu', 'calgary', 'alt.security', 'primarily', 'intelligent', 'eight', 'impression', 'chain', 'extermination', 'pound', 'chosen', 'smoke', 'linus.mitre.org', 'hopefully', 'fifth', 'cellular', 'bruins', 'strnlght', 'reform', 'campaign', 'practical', 'consequence', 'operate', 'motion', 'mormon', 'gather', 'prohibit', 'helpful', 'parameter', 'economy', 'comet', 'vnet.ibm.com', 'sam', 'printf', 'plastic', 'theist', 'accuse', 'on-line', 'ann', 'anthony', 'perspective', 'era', '72', 'aaron', 'carolina', 'wiring', 'opening', 'planetary', 'linux', 'tonight', 'physician', 'hiv', 'prime', 'adult', 'cloud', 'effectively', 'sp', 'eliminate', 'wing', 'crazy', 'picked', 'jhunix.hcf.jhu.edu', 'northern', 'henrik', 'buck', 'ethernet', 'yfn.ysu.edu', 'offensive', 'luke', 'acid', 'fat', 'constant', 'i.e.', 'republic', 'regulation', 'setting', 'ut', 'athens', 'ra', 'widely', 'february', 'driven', 'appeal', 'bullet', 'favorite', 'dispute', 'arizona', 'patent', 'resurrection', 'inform', 'operator', 'quebec', 'dns1.nmsu.edu', 'tommy', 'telescope', 'pens', 'colorado.edu', 'introduction', 'politics', 'whenever', 'hsdndev', 'bnrgate', 'declare', 'lynx.unm.edu', 'warwick', 'nyx', 'relation', 'struggle', 'fed', 'uchinews', 'unlike', 'kings', 'permission', 'un', 'viewer', 'bag', 'i/o', 'jays', 'goalie', 'hallam-baker', 'ra.royalroads.ca', 'honor', 'chief', 'fourth', '57', 'taste', 'democracy', 'msuinfo', 'compute', 'interactive', '79', 'todd', 'tough', 'ac', 'tb', 'nyi', 'december', 'france', '2.2', 'supporter', 'improvement', 'hide', 'headwall.stanford.edu', 'surround', 'august', 'aim', 'spell', 'gon', 'decade', 'news.cs.brandeis.edu', 'catch', 'jets', 'behanna', 'ripem', 'mainly', 'metal', 'indicates', 'rpi.edu', 'nazis', 'vary', 'confirm', 'asking', 'rumor', '1982', 'wa', 'july', 'ultra', 'cylinder', 'socket', 'hpscit.sc.hp.com', 'steer', 'sox', 'plenty', 'receiver', 'wonderful', 'camp', 'quantum', 'media', 'reverse', 'torture', '58', 'lost', 'syndrome', 'detailed', 'blind', 'knife', 'gift', 'opposite', 'asuvax', 'bitnet', 'married', 'propaganda', 'brett', 'stuck', '4th', 'captain', 'km', 'zealand', 'mojo.eng.umd.edu', 'vlb', 'sharks', 'x-soviet', 'premise', 'extreme', 'selection', 'fails', 'dear', 'assembly', 'sounds', 'corp', 'princeton.edu', 'skill', 'universal', 'slaughter', 'pool', 'guard', 'r5', 'pitt.uucp', 'clh', 'anatolia', 'expires', 'contradiction', 'rational', 'tomorrow', 'orthodox', 'mathematics', 'resident', 'curve', 'cbnewsj.cb.att.com', 'wise', 'abc', 'malcolm', 'oracle', 'utah', 'svga', '\\\\\\\\', 'adobe', 'aludra.usc.edu', 'undergrad.math.uwaterloo.ca', 'comp.security.misc', 'rwing.uucp', 'promote', 'eastern', 'idle', 'hat', 'viking', 'election', 'exodus.eng.sun.com', 'publication', 'significantly', 'saturday', 'barry', \"'92\", 'nelson', '/\\\\', 'cnn', 'clarinet.com', 'roby', 'azerbaijani', 'being', 'understood', 'ignorant', 'usenet.ufl.edu', 'forum', 'grade', 'cap', 'alexia.lis.uiuc.edu', 'messiah', 'timothy', 'critical', 'weekend', 'film', 'hotel', 'investigate', 'educational', 'thunder.mcrcim.mcgill.edu', 'expo.lcs.mit.edu', 'pete', 'apps', 'athena.cs.uga.edu', 'devils', '^_o', 'describes', '+44', 'bobby', 'news.ans.net', 'vol', 'mix', 'husband', 'lift', 'broke', 'mo', 'vesa', 'demo', '81', 'highway', 'nra', 'miracle', 'pack', 'ccwf.cc.utexas.edu', 'useless', 'typically', 'numerous', 'ottoman', 'news.claremont.edu', 'amaze', 'therapy', 'flash', 'demon', '\\\\_', 'cellar.org', 'nubus', 'das.harvard.edu', '1920', 'stl', 'suddenly', 'campus', 'vice', 'presumably', 'dozen', 'elect', 'ah', 'huh', 'essential', 'belong', 'leg', 'grand', '63', 'superior', 'bb', 'se', 'oasys.dt.navy.mil', 'mov', 'reno', 'bat', 'si', 'scsi-1', 'cs.rochester.edu', 'justification', 'similarly', 'famous', 'ideal', 'jump', 'affair', 'furthermore', 'doc', 'flat', '78', 'hook', 'broadcast', 'ms.', 'gang', 'julian.uwo.ca', '77', 'gold', 'atom', 'empty', 'priest', 'allan', 'idiot', 'subscribe', 'daemon', 'cash', 'accomplish', 'correction', 'ot', 'terrible', 'silver', 'usenet.coe.montana.edu', 'gumby', 'nsmca', '1.0', '1.2', 'sake', 'divide', 'classic', 'so-called', '/|', '1987', 'strategy', 'mt', 'news.ysu.edu', 'co.', 'partner', 'export.lcs.mit.edu', 'alt.sex', 'homicide', 'concentrate', 'catalog', 'physic', 'encounter', 'nntp-server.caltech.edu', 'electric', 'ctron-news.ctron.com', 'rape', 'humanity', 'phase', 'elsewhere', 'quack', 'occupation', 'classify', 'portal', 'veal', 'carson.u.washington.edu', 'jupiter', '1983', 'manufacture', 'utarlg.uta.edu', 'tap', 'williams', 'mag', 'clinical', 'temple', '_the', 'as', 'albert', 'prison', 'insult', 'crap', 'bound', 'supreme', 'janet', 'route', 'roman', 'liar', 'labs', 'spare', 'skin', 'hmmm', 'specification', '241-9760', 'app', 'cactus.org', 'dma', 'vram', 'cubs', 'square', 'punish', 'nonsense', 'signature', 'river', 'rice', 'temporary', 'smart', 'gallant.apple.com', 'greeks', 'january', 'led', 'super', '650', 'brent', 'inning', 'alt.privacy.clipper', 'fallacy', 'slip', 'hebrew', 'cultural', 'virtually', 'monday', 'sacrifice', \"'em\", 'sean', '*****', 'menudo.uh.edu', 'restore', 'islanders', 'stealth', 'minimum', 'silicon', 'passenger', 'rare', 'duke', 'contribution', 'extent', 'democratic', 'expression', 'employee', '-|', 'relate', 'bosnia', 'forth', 'competition', 'warranty', 'csn', 'acs.ucalgary.ca', '1919', 'talk.politics.soviet', 'substance', 'odd', 'atheists', 'traditional', 'apparent', 'ridiculous', '73', 'stable', 'forgot', 'museum', 'iowa', '95', 'colormap', 'mm', 'grenade', 'sumgait', 'hidden', 'passing', 'believer', 'acts', 'opinions', 'blah', 'insist', 'preserve', 'revolution', 'bureau', 'meter', 'pure', 'minister', 'edmonton', 'sunos', 'filename', 'cartridge', 'convex.com', 'ozone', 'tapped', 'antenna', 'adl', 'penguins', 'walter', 'assist', 'union', 'destruction', 'interrupt', 'virus', 'germany.eu.net', 'pyramid', 'edm', 'jackson', 'burns', 'county', '1986', 'dick', 'wisconsin', 'ought', 'drawn', 'incorrect', 'talent', 'empire', 'photo', 'avenue', 'scanner', 'gnv.ifas.ufl.edu', '2.1', 'weird', 'genesis.mcs.com', 'dbd', 'behaviour', 'salvation', 'comments', 'disciple', 'uniform', 'eg', 'teaching', 'gamma', 'permanent', 'flag', 'clue', 'fell', 'cook', 's1', 'suite', 'toyota', 'key-escrow', 'batman.bmd.trw.com', 'jaeger', 'annoy', 'kmr4', 'rocket.sw.stratus.com', 'membership', '62', 'resolve', 'decrease', 'difficulty', 'laboratories', '*is*', 'exploration', 'employ', 'optional', 'deficit', 'ta', 'arrogance', 'ml', 'awful', 'thanx', '415', 'dartvax.dartmouth.edu', 'rating', 'snow', 'phillies', 'immediate', 'compete', 'husc-news.harvard.edu', 'news+', 'thrown', 'bnr.co.uk', 'invent', 'massive', 'att-out', 'celebrate', 'artificial', 'strength', 'tactic', 'digest', 'assistance', 'rev', '/_', 'looking', 'developer', 'threw', 'spy', 'sabbath', 'hughes', '32-bit', 'lane', 'flames', 'ranch', 'boi.hp.com', 'saint', 'derive', 'pennsylvania', 'initiative', 'script', 'settle', 'flee', 'romans', 'visualization', 'norton', 'murphy', 'utzoo', 'clem.handheld.com', 'newer', 'milwaukee', 'entity', 'monu6.cc.monash.edu.au', 'horse', 'e.g.', 'regional', 'bradley', 'cnsvax.uwec.edu', 'compiler', 'pipe', 'verify', 'till', 'memorial', 'zeus.calpoly.edu', 'identical', 'rifle', 'shock', 'morrow.stanford.edu', 'adjust', 'chinese', 'nyc', 'agenda', '71', 'cursor', 'pacific', 'height', 'chopin.udel.edu', 'lemieux', 'spectrum', 'spiritual', 'harder', 'guaranteed', 'closely', 'reasonably', 'infant', 'complicate', 'constitutional', 'quiet', 'vm1.mcgill.ca', 'beast', 'investment', 'nine', 'kpc.com', 'technologies', 'x11r4', 'martha.utcc.utk.edu', 'kaldis', 'zionism', 'theism', 'appearance', 'burden', 'tektronix', 'creationism', 'unusual', 'quit', 'bed', 'remind', 'conservative', 'seizure', 'consent', 'interview', 'shadow', 'unixg.ubc.ca', 'timing', 'tip', 'neal', 'sensitivity', 'sparc', 'products', 'bigboote.wpi.edu', 'u.s.a.', 'wright', 'a86', 'desire.wright.edu', 'bontchev', 'ca.politics', 'purely', 'glory', 'analogy', 'mess', 'princeton', 'implication', 'ursa.bear.com', \"can't\", 'dr', '.sig', 'heck', 'worker', 'tuesday', 'cure', 'ear', 'angle', 'vitamin', 'approximately', 'mechanical', 'jan', 'pacbell.com', 'mizzou1.missouri.edu', 'batting', 'jr.', 'legally', 'chose', 'entitle', 'rambo.atlanta.dg.com', 'leadership', '-0600', 'precisely', 'valuable', 'justified', 'paladin.american.edu', 'severe', 'attend', 'po', 'scholar', 'bryan', 'horizon', 'bombing', '84', '6.0', 'ian', 'recommendation', 'configure', '92', 'cs.brown.edu', 'hess', 'simm', 'timer', 'shaft', 'surgery', 'alomar', 'listing', 'everywhere', 'schneider', 'rawlins', 'dont', 'non', 'concrete', 'freely', 'translate', 'identity', 'assure', 'discrimination', 'teacher', 'beach', 'east.sun.com', '89', '1.5', '600', 'enable', 'lll-winken.llnl.gov', 'password', 'delivery', 'victoria', 'richardson', 'adaptec', 'gant', 'amanda', '88', 'bobbe', 'proud', 'hmm', 'netters', 'trash', 'eh', 'genesis', 'pl6', 'suggests', 'disappear', 'commitment', '120', 'acknowledge', 'orbital', 'researcher', 'ulysses.att.com', 'unique', 'jay', 'bounce', '5.0', '68', 'mitchell', 'msuinfo.cl.msu.edu', 'mercury', 'yassin', 'books', 'hunt', 'hopkins', 'friendly', 'gregg', 'ordinary', 'corrupt', 'television', 'einstein', 'hacker', '67', 'acquire', 'silence', 'dry', 'utcsri', 'spoke', 'weather', '8-bit', 'scope', 'machines', 'higgins', 'mamma', 'explicitly', 'adopt', 'experimental', 'experienced', 'native', '____', 'eczcaw', 'mips.nott.ac.uk', 'sei.cmu.edu', 'covenant', \"'i\", 'rent', 's2', 'arthur', 'deliver', 'marketing', 'beta', 'ltd.', 'custom', 'thf2', 'livni', 'news.answers', 'trap', 'institution', 'flaw', 'nose', 'criterion', 'freeman', 'clone', 'worried', '99', 'howard', 'le', 'sb', 'rose', 'mlee', 'applications', 'samba.oit.unc.edu', 'oregon', 'mp', 'sc', 'eliot', 'ama', 'secular', 'comprehensive', 'construct', 'objectively', 'construction', 'legislation', 'joel', 'brief', 'dynamic', \"'91\", 'download', 'hire', 'rd', 'giants', 'rec.autos.tech', 'golchowy', 'comp.org.ieee', 'twist', 'serf', 'mental', 'mathematical', 'arrogant', 'objection', 'ousrvr.oulu.fi', 'capacity', '74', 'stupidity', 'regularly', 'invade', 'defeat', 'inhabitant', 'demon.co.uk', 'sponsor', 'availability', '130', 'callison', 'hernlem', 'writing', 'news.bbn.com', 'stan', 'commonly', 'better', 'infinite', 'qualify', 'starter', 'farm', 'jefferson', 'scene', 'syria', 'enhance', 'array', 'ms-dos', 'manipulation', 'nl', '617', 'junk', 'irq', 'scratch', 'ubvmsb.cc.buffalo.edu', 'zoology', 'comp.org.acm', 'mcovingt', 'mystery', 'mimsy.umd.edu', 'slide', 'tavares', 'stratus', 'rushdie', 'angel', 'whereas', 'industrial', 'grab', 'zionist', 'beating', 'hatch', 'bh', 'shield', 'exhaust', 'programs', 'alexander', 'u.s', 'mccall', 'launcher', 'iisi', 'eff.org', 'bonds', 'soc.culture.greek', 'kinsey', 'devil', 'whoever', 'naturally', 'characteristic', 'probability', 'charlie', 'suitable', 'rough', 'measurement', 'conspiracy', 'ga', 'prince', 'ratio', 'govt', 'iv', 'lean', 'import', 'waikato.ac.nz', 'ra.msstate.edu', 'startup', 'analog', 'hal', 'joint', 'beck', 'div', 'scorer', 'pts', 'expense', 'tamsun.tamu.edu', 'el', 'discount', 'consult', 'october', 'negotiate', 'muscle', \"i've\", 'poll', 'graduate', 'sword', 'ye', 'paranoid', 'cunews', 'alberta', 'andre', 'graph', 'gt', 'stsci.edu', 'comp.os.ms-windows.apps', 'hamilton', 'hm', 'col.hp.com', 'factory', 'oname', 'sabres', 'invasion', 'confusion', 'stretch', 'josh', 'gee', 'prize', 'female', 'impose', 'absurd', 'titan', 'opponent', 'nut', 'harvey', 'stat', '86', 'neil', 'thursday', 'protest', 'volunteer', 'executive', 'senator', 'infrastructure', 'dallas', '69', 'naval', 'radius', 'macs', 'photography', 'gerry', 'palmer', 'premium', 'xfree86', 'selanne', 'revolver', 'nyr', 'hicnet', 'tclock', 'istanbul', '700', 'uh', 'tends', 'roughly', 'belt', 'alot', 'relief', 'thru', 'profit', 'instruments', 'rogers', 'x-disclaimer', 'prism', 'cc', 'batter', 'ahl', 'denning', 'noring', 'soc.men', 'britain', 'opposition', '05', 'mexico', 'ascii', 'fanatic', 'ethnic', 'ira', 'warm', 'invite', 'discharge', '87', 'sue', 'warren', 'uunet.uu.net', 'presentation', 'seas.gwu.edu', 'ya', 'switzerland', 'arc', 'bulletin', '4.0', 'rahul.net', 'ux4.cso.uiuc.edu', 'forsale', '91', 'valve', 'fiction', 'isbn', 'acceptance', 'speculation', 'strictly', 'environmental', 'soft', 'upset', 'evans', 'distinction', 'indication', 'particle', 'neat', 'reward', 'hr', 'affected', 'delay', 'anonymity', 'comic', 'royal', 'dumb', 'jpl', 'ir', 'sept.', 'egreen', 'prb', 'jmd', 'monthly', 'row', 'horrible', 'rain', 'motivate', 'works', 'fluid', 'sum', 'sight', 'ministry', 'tobacco', 'terry', 'rm', 'collins', 'ms.uky.edu', 'dir', 'da', 'advertising', 'powerbook', 'parker', 'news-mail-gateway', 'alt.activism', 'infante', 'magpie.linknet.com', 'decrypt', 'covington', 'ultimate', 'intellectual', 'theology', 'quantity', 'magic', 'cage', 'wash', 'nowhere', 'orientation', 'labor', 'built-in', 'calculate', 'logo', 'alcohol', 'aircraft', 'pioneer', 'gl', 'games', 'chen', 'guideline', '1980', 'news.hawaii.edu', 'runner', 'mets', 'vms.ocom.okstate.edu', 'ohanus', 'mit.edu', 'atomic', 'differ', 'cs.umd.edu', 'news.funet.fi', 'blast', '06', 'commandment', 'coat', 'mario', 'loose', 'versus', 'ics.uci.edu', 'buyer', 'winter', '83', 'fl', 'upper', 'shearson.com', 'bloom', 'aio.jsc.nasa.gov', 'intergraph', '00', 'wednesday', 'speedy', 'resistor', 'weaver', \"'we\", 'appressian', 'soc.history', 'psychology', 'implies', 'automobile', 'blank', 'fuck', 'odin', 'fido', 'feeling', 'arbor', 'enforce', 'exclude', 'raw', '1985', 'largely']\n"
     ]
    }
   ],
   "source": [
    "features = get_features(most_common_words)                   # extraction of features (vocabulary)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(len(features))                                         # verification : num_of_features = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = os.listdir(parent_directory)                       # the 20 categories (output)\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * getting the cleaned text to **X, Y** format, as used by **sklearn** classifiers\n",
    "> * generating **X_train, X_test, Y_train, Y_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = makeXY(features, parent_directory, stops)                     # List of lists for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest, Ytest = makeXY_test(features, parent_directory, stops)                  # List of lists for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to NumPy arrays\n",
    "\n",
    "X_train = np.array(Xtrain)                                   # X_train.shape = (16000, 4000)\n",
    "Y_train = np.array(Ytrain)                                   # Y_train.shape = (16000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(Xtest)                                     # X_test.shape = (3997, 4000)\n",
    "Y_test = np.array(Ytest)                                     # Y_test.shape = (3997, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping to get 1 column                       \n",
    "\n",
    "Y_train = Y_train.reshape(-1, 1)                             # Y_train.shape = (16000, 1)\n",
    "Y_test = Y_test.reshape(-1, 1)                               # Y_test.shape = (3997, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 1)\n",
      "(3997, 1)\n",
      "___________\n",
      "(16000, 4000)\n",
      "(3997, 4000)\n",
      "___________\n",
      "[26 97 91 ...,  0  0  0]\n",
      "[4 0 0 ..., 0 0 0]\n",
      "___________\n",
      "(4000,)\n",
      "(4000,)\n",
      "___________\n"
     ]
    }
   ],
   "source": [
    "# verification\n",
    "                  \n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)  \n",
    "print(\"___________\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(\"___________\")\n",
    "\n",
    "print(X_train[1])\n",
    "print(X_test[1])\n",
    "print(\"___________\")\n",
    "\n",
    "print(X_train[1].shape)\n",
    "print(X_test[1].shape)\n",
    "print(\"___________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > using the inbuilt **MultinomialNB()** classifier from **sklearn.naive_bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()                                        # creation of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)                                    # fit training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 1)\n",
      "(16000, 1)\n",
      "0.8843125\n",
      "0.8843125\n"
     ]
    }
   ],
   "source": [
    "Y_train_pred = clf.predict(X_train)                          # predict on training data        \n",
    "Y_train_pred = Y_train_pred.reshape(-1, 1)\n",
    "\n",
    "print(Y_train_pred.shape)                                    # verification\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(clf.score(X_train, Y_train))                           # score\n",
    "print(accuracy_score(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3997, 1)\n",
      "(3997, 1)\n",
      "0.795346509882\n",
      "0.795346509882\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = clf.predict(X_test)                            # predict on testing data\n",
    "Y_test_pred = Y_test_pred.reshape(-1, 1)   \n",
    "\n",
    "print(Y_test_pred.shape)                                     # verification\n",
    "print(Y_test.shape)\n",
    "\n",
    "print(clf.score(X_test, Y_test))                             # score\n",
    "print(accuracy_score(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> using the code written on my own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting training data\n",
    "\n",
    "dictionary = fit(X_train, Y_train)                           # dictionary of frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"total_docs\"]                                     # verification\n",
    "dictionary[classes[4]][\"total_class_docs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = predict(dictionary, X_test)                    # predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.70      0.68      0.69       200\n",
      "           comp.graphics       0.65      0.72      0.69       200\n",
      " comp.os.ms-windows.misc       0.84      0.69      0.75       200\n",
      "comp.sys.ibm.pc.hardware       0.74      0.78      0.76       200\n",
      "   comp.sys.mac.hardware       0.80      0.81      0.81       200\n",
      "          comp.windows.x       0.61      0.68      0.64       200\n",
      "            misc.forsale       0.74      0.89      0.81       200\n",
      "               rec.autos       0.82      0.92      0.87       200\n",
      "         rec.motorcycles       0.91      0.87      0.89       200\n",
      "      rec.sport.baseball       0.88      0.88      0.88       200\n",
      "        rec.sport.hockey       0.95      0.84      0.90       200\n",
      "               sci.crypt       0.95      0.88      0.91       200\n",
      "         sci.electronics       0.81      0.77      0.79       200\n",
      "                 sci.med       0.98      0.85      0.91       200\n",
      "               sci.space       0.90      0.92      0.91       200\n",
      "  soc.religion.christian       0.94      1.00      0.97       197\n",
      "      talk.politics.guns       0.65      0.69      0.67       200\n",
      "   talk.politics.mideast       0.87      0.86      0.86       200\n",
      "      talk.politics.misc       0.73      0.60      0.66       200\n",
      "      talk.religion.misc       0.55      0.58      0.57       200\n",
      "\n",
      "             avg / total       0.80      0.80      0.80      3997\n",
      "\n",
      "[[136   0   1   0   0   1   0   0   2   0   0   0   2   0   1   2   7   1\n",
      "    1  46]\n",
      " [  3 145   5   6   2  17   5   2   0   1   0   3   2   0   8   0   1   0\n",
      "    0   0]\n",
      " [  0   9 137  18   8  24   2   0   0   0   0   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   3 155  12   7   8   1   1   0   0   0  10   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   2  11 163   6   8   2   1   0   0   0   2   0   2   0   1   0\n",
      "    1   1]\n",
      " [  0  32   9   7   8 135   2   3   2   0   0   0   1   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   3   3   2   3 178   3   1   1   1   0   3   0   0   1   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   3   5 183   2   0   1   0   3   0   0   0   1   0\n",
      "    1   0]\n",
      " [  0   1   1   0   0   4   7   4 174   0   0   0   2   0   0   0   3   2\n",
      "    0   2]\n",
      " [  0   1   1   0   1   4   4   2   1 176   4   0   0   0   1   0   3   0\n",
      "    1   1]\n",
      " [  0   0   0   0   0   0   3   2   1  19 169   1   1   0   0   0   1   0\n",
      "    0   3]\n",
      " [  1   3   1   3   1   1   4   0   0   1   0 176   2   0   1   0   5   0\n",
      "    1   0]\n",
      " [  0   6   1   4   3  10   5  10   1   0   0   3 153   2   1   0   1   0\n",
      "    0   0]\n",
      " [  5   5   0   0   0   1   4   3   0   0   2   0   4 170   2   0   1   2\n",
      "    1   0]\n",
      " [  0   4   0   0   0   3   0   1   0   0   0   0   1   0 184   0   2   0\n",
      "    2   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 197   0   0\n",
      "    0   0]\n",
      " [  0   5   0   2   1   0   2   4   3   0   0   1   2   1   0   0 139   5\n",
      "   21  14]\n",
      " [  5   1   0   1   1   0   3   2   1   0   0   0   0   0   0   1   3 172\n",
      "   10   0]\n",
      " [  0   1   0   0   1   1   2   0   1   1   0   0   0   0   2   0  31  15\n",
      "  121  24]\n",
      " [ 44   4   0   0   1   2   0   1   0   0   0   0   0   0   1   9  15   1\n",
      "    6 116]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_test_pred))            # performance\n",
    "print(confusion_matrix(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795346509882\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(Y_test_pred, Y_test))                   # score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
